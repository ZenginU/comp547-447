{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/johnberg1/COMP447-547-S22/blob/main/homework1/HW1_Autoregressive_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment1: Autoregressive Models\n",
    "### Due on March 15, 2024 (23:59:59)\n",
    "\n",
    "Welcome to comp547!\n",
    "\n",
    "The main goal of this practical is to make you understand the basics of autoregressive models, and you will also get familiar with Facebook’s Pytorch deep learning framework. In this practical you will work on some basic histograms, PixelCNN, and iGPT. At the end of the each part, you will have an inline question that requires a written answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Things First: Academic Integrity for Homework1\n",
    "\n",
    "In our Deep Unsupervised Learning course, all work on assignments must be done individually unless stated otherwise. You can make discussions in an \"abstract\" way. However, it's crucial to distinguish between constructive discussions and the inappropriate use of resources: turning in someone else’s or your friend's work, in whole or in part, as your own will be considered as a violation of academic integrity. Please note that the former condition also holds for the material found on the web as everything on the web has been written by someone else. In short, using code from your peers or from any online sources as your own work **will not be tolerated**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rdy1FtrRpGcC"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "## Overview\n",
    "This semester, all homeworks will be conducted through Google Colab notebooks. All code for the homework assignment will be written and run in this notebook. Running in Colab will automatically provide a GPU, but you may also run this notebook locally by following [these instructions](https://research.google.com/colaboratory/local-runtimes.html) if you wish to use your own GPU.\n",
    "\n",
    "You will save images in the notebooks to use and fill out a given LaTeX template which will be submitted to Blackboard, along with your notebook code.\n",
    "\n",
    "## Using Colab\n",
    "On the left-hand side, you can click the different icons to see a Table of Contents of the assignment, as well as local files accessible through the notebook.\n",
    "\n",
    "Make sure to go to **Runtime -> Change runtime type** and select **GPU** as the hardware accelerator. This allows you to use a GPU. Run the cells below to get started on the assignment. Note that a session is open for a maximum of 12 hours, and using too much GPU compute may result in restricted access for a short period of time. Please start the homework early so you have ample time to work.\n",
    "\n",
    "**If you loaded this notebook from clicking \"Open in Colab\" from github, you will need to save it to your own Google Drive to keep your work.**\n",
    "\n",
    "## General Tips\n",
    "In the first problem, you will implement an autoregressive model and run it on two datasets (dataset 1 and dataset 2). The expected outputs for dataset 1 are already provided to help as a sanity check. In the second problem, you will train on one dataset (MNIST).\n",
    "\n",
    "After each part, there will be a written question that you will need to answer about that part. These questions are given under the <font color='red'> Inline Question </font> title. You just need to write your answers below those questions.\n",
    "\n",
    "Feel free to print whatever output (e.g. debugging code, training code, etc) you want. Your graded submission will be the report you submit together with a copy of your notebook.\n",
    "\n",
    "You will be implementing the code using PyTorch. Some parts of the code such as function headers are already provided to you, you will need to fill out the parts with \"YOUR CODE HERE\". Feel free to create more cells and more functions or modify the existing ones if you wish.\n",
    "\n",
    "After you complete the assignment, download all of the image outputted in the results/ folder (which you can find on the left dashboard by clicking the folder icon) and upload them to the figure folder in the given latex template.\n",
    "\n",
    "Run the cells below to download and load up the starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUVy2glDtoaR"
   },
   "outputs": [],
   "source": [
    "!if [ -d deepul ]; then rm -Rf deepul; fi \n",
    "!git clone https://github.com/emrecanacikgoz/deepul.git\n",
    "!unzip -qq deepul/homeworks/hw1/data/hw1_data.zip -d deepul/homeworks/hw1/data/\n",
    "!pip install ./deepul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHWosWrbpO5Y"
   },
   "outputs": [],
   "source": [
    "from deepul.hw1_helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7E4CMktzo100"
   },
   "source": [
    "# [25 Points] Question 1: 1D Data\n",
    "\n",
    "In this question, we will train simple generative models on discrete 1D data.\n",
    "\n",
    "Execute the cell below to visualize our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ehhv2FZGo4_b"
   },
   "outputs": [],
   "source": [
    "visualize_q1_data(dset_type=1)\n",
    "visualize_q1_data(dset_type=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSGTVznZqAR3"
   },
   "source": [
    "## [10 Points] Part (a) Fitting a Histogram\n",
    "\n",
    "Let $\\theta = (\\theta_0, \\dots, \\theta_{d-1}) \\in \\mathbb{R}^d$ and define the model $p_\\theta(x) = \\frac{e^{\\theta_x}}{\\sum_{x'}e^{\\theta_{x'}}}$\n",
    "\n",
    "Fit $p_\\theta$ with maximum likelihood via stochastic gradient descent on the training set, using $\\theta$ initialized to zero. Use your favorite version of stochastic gradient descent, and optimize your hyperparameters on a validation set of your choice.\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. Plot the model probabilities in a bar graph with $\\{0,\\dots,d-1\\}$ on the x-axis and a real number in $[0,1]$ on the y-axis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg0Jmo1PSaE4"
   },
   "source": [
    "### Solution\n",
    "Fill out the functions below and return the necessary arguments. Feel free to create more cells if need be.\n",
    "\n",
    "Implement the Histogram class which extends nn.Module, you will need to create the parameters of the model and provide a loss given a batch of data. In the get_distribution function, you will return a valid probability distribution which sums up to 1. You may want to take a look at the documentations of torch.nn.functional.cross_entropy() and torch.nn.functional.softmax()\n",
    "\n",
    "After creating your model, you will train your model on the training set and evaluate on the test set. \n",
    "\n",
    "**Hint**: You may want to implement training and the evaluation procedures as functions which take a model and the dataloaders as an input and return the losses. This way, you can use these functions to train all the models you will create in this homework by simply calling them from inside the necessary functions such as 'q1_a' or 'q1_b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkZoFkhQeOf3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ycAfjmAfLQw"
   },
   "outputs": [],
   "source": [
    "class Histogram(nn.Module):\n",
    "  def __init__(self, d):\n",
    "    super().__init__()\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def loss(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def get_distribution(self):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BJNa6dHKpEQU"
   },
   "outputs": [],
   "source": [
    "def q1_a(train_data, test_data, d, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: An (n_train,) numpy array of integers in {0, ..., d-1}\n",
    "  test_data: An (n_test,) numpy array of integers in {0, .., d-1}\n",
    "  d: The number of possible discrete values for random variable x\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "             used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (d,) of model probabilities\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiGBSP-ESeIj"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q1_a`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "qjK_KReXsqYa",
    "outputId": "fe6ae57d-63d8-4e50-a3ed-b0fb087a633f"
   },
   "outputs": [],
   "source": [
    "q1_save_results(1, 'a', q1_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJVOUEaaZXcA"
   },
   "outputs": [],
   "source": [
    "q1_save_results(2, 'a', q1_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnxld_vbh17F"
   },
   "source": [
    " ### <font color='red'> Inline Question 1 </font>\n",
    "\n",
    "*   What is the number of parameters of your Histogram model, for dataset 1 and dataset 2?\n",
    "*   Supose that you are implementing a Histogram to model 28x28 binary images (each pixel has value of {0,1}). How many parameters would this model need to have? Is this a feasible way to model image distributions?\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiyFXlj0rfcr"
   },
   "source": [
    "## [10 Points] Part (b) Fitting Discretized Mixture of Logistics\n",
    "\n",
    "Let us model $p_\\theta(x)$ as a **discretized** mixture of 6 logistics such that $p_\\theta(x) = \\sum_{i=1}^6 \\pi_i[\\sigma((x+0.5 - \\mu_i)/s_i) - \\sigma((x-0.5-\\mu_i)/s_i)]$\n",
    "\n",
    "For the edge case of when $x = 0$, we replace $x-0.5$ by $-\\infty$, and for $x = 99$, we replace $x+0.5$ by $\\infty$.\n",
    "\n",
    "You may find the [PixelCNN++](https://arxiv.org/abs/1701.05517) helpful for more information on discretized mixture of logistics.\n",
    "\n",
    "**Provide the same set of corresponding deliverables as part (a)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4dnQIg_TDx6"
   },
   "source": [
    "### Solution\n",
    "Fill out the functions below and return the necessary arguments. Feel free to create more cells if need be. \n",
    "\n",
    "Implement the MixtureOfLogistics class which extends nn.Module. Similar to the first part, you will create the parameters, implement the forward pass, loss functions and provide a function which returns a valid probability distribution.\n",
    "\n",
    "Possible functions that might help: torch.sigmoid(), torch.clamp(), F.log_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3hX-sb-kHB7"
   },
   "outputs": [],
   "source": [
    "class MixtureOfLogistics(nn.Module):\n",
    "  def __init__(self, d, n_mix=6):\n",
    "    super().__init__()\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def loss(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def get_distribution(self):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAvMQDJJrjNo"
   },
   "outputs": [],
   "source": [
    "def q1_b(train_data, test_data, d, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: An (n_train,) numpy array of integers in {0, ..., d-1}\n",
    "  test_data: An (n_test,) numpy array of integers in {0, .., d-1}\n",
    "  d: The number of possible discrete values for random variable x\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (d,) of model probabilities\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwZyhlewTHH4"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q1_b`, execute the cells below to visualize and save your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "wnnQORaG6Ouf",
    "outputId": "3890f9ce-84cb-4c48-f2ca-9a1746acc424"
   },
   "outputs": [],
   "source": [
    "q1_save_results(1, 'b', q1_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jLGoDa46RM6"
   },
   "outputs": [],
   "source": [
    "q1_save_results(2, 'b', q1_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drZR4G6hj3Ki"
   },
   "source": [
    " ### <font color='red'> Inline Question 2 </font>\n",
    "\n",
    "*   Please run the experiments **on dataset 2** with `n_mix = 2` and `n_mix = 4` again and observe the changes in the results. Are your results as good as the results with `n_mix = 6`? Why do you think this happens? Please explain briefly. (Note that for the report, **you need to include the results for** `n_mix = 6` **only**, the other experiments are just for you to observe).\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiIU3aRxZ685"
   },
   "source": [
    "# [45 Points] Question 2 PixelCNNs\n",
    "\n",
    "Now, you will train more powerful PixleCNN models on the shapes dataset and MNIST. In addition, we will extend to modelling colored datasets with and without channel conditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 Points] Part (a) PixelCNN on Shapes and MNIST\n",
    "In this part, implement a simple PixelCNN architecture to model binary MNIST and shapes images (same as Q2(b), but with a PixelCNN).\n",
    "\n",
    "We recommend the following network design:\n",
    "* A $7 \\times 7$ masked type A convolution\n",
    "* $5$ $7 \\times 7$ masked type B convolutions\n",
    "* $2$ $1 \\times 1$ masked type B convolutions\n",
    "* Appropriate ReLU nonlinearities in-between\n",
    "* 64 convolutional filters\n",
    "\n",
    "And the following hyperparameters:\n",
    "* Batch size 128\n",
    "* Learning rate $10^{-3}$\n",
    "* 10 epochs\n",
    "* Adam Optimizer (this applies to all PixelCNN models trained in future parts)\n",
    "\n",
    "Your model should output logits, after which you could apply a sigmoid over 1 logit, or a softmax over two logits (either is fine). It may also help to scale your input to $[-1, 1]$ before running it through the network. \n",
    "\n",
    "Training on the shapes dataset should be quick, and MNIST should take around 10 minutes\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jsCqLJwezLq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import sys\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDL-G8yMeK9z"
   },
   "source": [
    "You will train your model on MNIST. Let's download the dataset and visualize an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "1EXaGq10eTG4",
    "outputId": "77df6a54-346c-42a2-8fcc-6406978bcd32"
   },
   "outputs": [],
   "source": [
    "dataset_path = '/content/data'\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root=dataset_path,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "test_data = datasets.MNIST(root=dataset_path,\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "print(train_data)\n",
    "\n",
    "plt.imshow(train_data.data[447], cmap='gray')\n",
    "plt.title('%i' % train_data.targets[447])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjKI3H6bxQE6"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iYrnz9ZaR1e"
   },
   "source": [
    "In this part, you will implement a very simple PixelCNN architecture on greyscale handwritten digits. This model will be trained with the default Cross Entropy Loss.\n",
    "\n",
    "As you have seen in the lecture, PixelCNN utilizes masked convolutions to ensure the autoregressive property is satisfied. First start by implementing a masked convolutional layer. Remember that there are two different types of layers, type A and type B, depending on the masking you are using.\n",
    "\n",
    "A class header is provided to you in the cell below, which extends the `nn.Conv2d` class. You may wish to implement your layer that extends the `nn.Module` class instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSnqP__sbjXF"
   },
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "  def __init__(self, mask_type, *args, **kwargs):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFBAigvBcMP9"
   },
   "source": [
    "Now that you have implemented a masked convolutional layer, you can use this layer inside your PixelCNN model. Again, a class header is provided to you. Here is a pseudocode for the reference implementation:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "class PixelCNN:\n",
    "  MaskedConv(mask_type='A', in_channels = 1, out_channels = 64, kernel_size = 7, stride = 1, padding = 3)\n",
    "  BatchNorm(num_features=64)\n",
    "  ReLU()\n",
    "\n",
    "  for _ in range(3):\n",
    "    MaskedConv(mask_type='B', in_channels = 64, out_channels = 64, kernel_size = 7, stride = 1, padding = 3)\n",
    "    BatchNorm(num_features=64)\n",
    "    ReLU()\n",
    "\n",
    "  MaskedConv(mask_type='B', in_channels = 64, out_channels = 64, kernel_size = 1)\n",
    "  BatchNorm(num_features=64)\n",
    "  ReLU()\n",
    "  MaskedConv(mask_type='B', in_channels = 64, out_channels = 256, kernel_size = 1)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YkxcukLckHT"
   },
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "  def __init__(self, num_layers = 5, num_channels = 64, kernel_size = 7):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lexYZvOIgWgs"
   },
   "source": [
    "Now you are ready to train your model. Fill in the function `q2_a` below to train the model and return the correct deliverables. In this function, you will initialize your model and train this model. The reference implementation uses `batch_size = 128`, `num_epocs = 5`, `optimizer = Adam`, `lr = 1e-3` as hyperparameters. Use Cross Entropy Loss to train this model, please look at the documentation of this loss. It should take around 16 minutes to train for 5 epochs\n",
    "\n",
    "**You will provide the following deliverables:**\n",
    "\n",
    "*   **A list of training losses evaluated at every minibatch. (Create an empty list before training, append the loss after every minibatch and return the final list).**\n",
    "*   **A list of test losses evaluated once at initialization and after each epoch. (Create an empty list. Evaluate on the test set before training and append it to this list. After every epoch, evaluate on the test set again and append to the list. Return the final list).**\n",
    "*   **25 samples drawn from your trained model. (The sampling function is already provided to you. Just call this function after training and return the samples).**\n",
    "\n",
    "Here is a pseudocode for the reference implementation. Once again, you may wish to implement a generic training function (`train_model`) that takes your model and dataloaders and returns the training losses:\n",
    "\n",
    "\n",
    "```\n",
    "function q2a():\n",
    "  batch_size = 128\n",
    "  num_epochs = 5\n",
    "  train_loader = data.DataLoader(train_data, ...)\n",
    "  test_loader = data.DataLoader(test_data, ...)\n",
    "  device = 'cuda:0' or 'cpu'\n",
    "  model = PixelCNN().to(device)\n",
    "  train_losses, test_losses = train_model(model, train_loader, test_loader, ...)\n",
    "  \n",
    "  samples = sample(model)\n",
    "  return train_losses, test_losses, samples\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pw_lqcTRiLeC"
   },
   "outputs": [],
   "source": [
    "# Execute this cell, nothing to implement here\n",
    "def sample(model):\n",
    "  \"\"\"\n",
    "  model: PixelCNN model trained on MNIST\n",
    "\n",
    "  Returns\n",
    "  - sample: A tensor containing the samples generated by the model\n",
    "  \"\"\"\n",
    "  \n",
    "  no_images = 25\n",
    "  images_channels = 1\n",
    "  images_size = 28\n",
    "  sample = torch.Tensor(no_images, images_channels, images_size, images_size).cuda()\n",
    "  sample.fill_(0)\n",
    "\n",
    "  for i in range(images_size):\n",
    "    for j in range(images_size):\n",
    "      out = model(sample)\n",
    "      probs = F.softmax(out[:,:,i,j], dim=-1).data\n",
    "      sample[:,:,i,j] = torch.multinomial(probs, 1).float() / 255.0\n",
    "  return sample.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jdg59OElghMf"
   },
   "outputs": [],
   "source": [
    "def q2_a(train_data, test_data):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W) numpy array of greyscale MNIST images\n",
    "  test_data: A (n_test, H, W) numpy array of greyscale MNIST images\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) list of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) list of test_losses evaluated once at initialization and after each epoch\n",
    "  - a tensor containing samples generated by the trained model\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNJjEMmxmKOT"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_a`, execute the cell below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQoj6As1o7uU"
   },
   "outputs": [],
   "source": [
    "q2a_save_results(q2_a, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPyDAkD7kTlN"
   },
   "source": [
    " ### <font color='red'> Inline Question 3 </font>\n",
    "\n",
    "*   Bits per dim is a common metric to evaluate these autoregressive models such as PixelCNN. Please describe what bits per dim is, what is its relationship with log-likelihood and how it is calculated.\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R928z8AgXa2d"
   },
   "source": [
    "## [15 Points] Part (b) PixelCNN+ on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9HjM0SVX_4U"
   },
   "source": [
    "In this part, you will implement the mixture of logistics used in PixelCNN++, instead of the cross entropy loss used in the original PixelCNN. We call this model PixelCNN+, since we are only adapting the loss function and not implementing the other details presented in PixelCNN++. Let's start by loading the dataset, this time we will normalize the pixels to be in the range [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPyMItkwnVw6"
   },
   "outputs": [],
   "source": [
    "dataset_path = '/content/data'\n",
    "rescaling     = lambda x : (x - .5) * 2.\n",
    "rescaling_inv = lambda x : .5 * x  + .5\n",
    "transform = transforms.Compose([transforms.ToTensor(), rescaling])\n",
    "train_data = datasets.MNIST(root=dataset_path,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "test_data = datasets.MNIST(root=dataset_path,\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "print(train_data)\n",
    "\n",
    "plt.imshow(train_data.data[447], cmap='gray')\n",
    "plt.title('%i' % train_data.targets[447])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jMRvOfTxT5l"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So9P0MWhcgF6"
   },
   "source": [
    "Please complete the `PixelCNNPlus` class below. Your model will be almost exactly the same with part (a), with a small modification on the last convolutional layer. Since we will output a mixture of logistics, instead of a 256 way softmax, the number of output channels should be equal to `3 * number_of_mixtures`. In the reference implementation, we use the `number_of_mixtures = 10`, therefore the last convolutional layer has `30` filters in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6hmQqVXp797"
   },
   "outputs": [],
   "source": [
    "class PixelCNNPlus(nn.Module):\n",
    "  def __init__(self, num_layers = 5, num_channels = 64, kernel_size = 7):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRnzCWxRc2Wl"
   },
   "source": [
    "\n",
    "The main implementation will be about the loss function. You need to implement **Discretized Logistic Mixture Likelihood** from PixelCNN++. Please take a look at the paper for the detailed explanation of this. Your implementation will be very similar to the Discretized Mixture of Logistics you implemented in Question 1 (b). However, here we will be working on images instead of 1-dimensional data. \n",
    "\n",
    "One important detail is that in this part, we normalize the data to be in the range [-1,1] when loading it, so the following function assumes that your data is in this range. Please fill out the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IERg3_yXdJgo"
   },
   "outputs": [],
   "source": [
    "def mix_of_logistics_loss(fake, real):\n",
    "  \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "incQii-vdm-c"
   },
   "source": [
    "Now, you need to use this loss as the loss function in your training loop, instead of the Cross Entropy Loss from part (a). You can borrow your previous training loops / functions, but you may need to do some small changes for compatibility of the losses etc.\n",
    "\n",
    "Fill in the function `q2_b` below to train the model and return the correct deliverables. In this function, you will initialize your model and train this model. The reference implementation uses `batch_size = 128`, `num_epocs = 5`, `optimizer = Adam`, `lr = 1e-3` as hyperparameters. It should take around 16 minutes to train for 5 epochs\n",
    "\n",
    "**You will provide the following deliverables:**\n",
    "\n",
    "*   **A list of training losses evaluated at every minibatch. (Create an empty list before training, append the loss after every minibatch and return the final list).**\n",
    "*   **A list of test losses evaluated once at initialization and after each epoch. (Create an empty list. Evaluate on the test set before training and append it to this list. After every epoch, evaluate on the test set again and append to the list. Return the final list).**\n",
    "*   **25 samples drawn from your trained model. (The sampling function is already provided to you. Just call this function after training and return the samples).**\n",
    "\n",
    "Here is a pseudocode for the reference implementation:\n",
    "\n",
    "\n",
    "```\n",
    "function q2b():\n",
    "  batch_size = 128\n",
    "  num_epochs = 5\n",
    "  train_loader = data.DataLoader(train_data, ...)\n",
    "  test_loader = data.DataLoader(test_data, ...)\n",
    "  device = 'cuda:0' or 'cpu'\n",
    "  model = PixelCNNPlus().to(device)\n",
    "  train_losses, test_losses = train_model(model, train_loader, test_loader, ...)\n",
    "  \n",
    "  samples = sample_mixture(model)\n",
    "  return train_losses, test_losses, samples\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLXjbAfxjGCU"
   },
   "outputs": [],
   "source": [
    "# Execute this cell, nothing to implement here\n",
    "def sample_mix(model):\n",
    "  \"\"\"\n",
    "  model: PixelCNNPlus model trained on MNIST\n",
    "\n",
    "  Returns\n",
    "  - sample: A tensor containing the samples generated by the model\n",
    "  \"\"\"\n",
    "  obs = (1, 28, 28)\n",
    "  sample_batch_size = 25\n",
    "  sample_op = sample_from_discretized_mix_logistic_1d\n",
    "  model.eval()\n",
    "  samples = torch.zeros(sample_batch_size, obs[0], obs[1], obs[2])\n",
    "  samples = samples.cuda()\n",
    "  for i in range(obs[1]):\n",
    "      for j in range(obs[2]):\n",
    "          samples_v = samples\n",
    "          out   = model(samples_v)\n",
    "          out_sample = sample_op(out, nr_mix=10)\n",
    "          samples[:, :, i, j] = out_sample.data[:, :, i, j]\n",
    "  return rescaling_inv(samples).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF-pStecjEbp"
   },
   "outputs": [],
   "source": [
    "def q2_b(train_data, test_data):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W) numpy array of greyscale MNIST images\n",
    "  test_data: A (n_test, H, W) numpy array of greyscale MNIST images\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) list of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) list of test_losses evaluated once at initialization and after each epoch\n",
    "  - a tensor containing samples generated by the trained model\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJK6USuDmHh1"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_b`, execute the cell below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxyl9h3jl_OJ"
   },
   "outputs": [],
   "source": [
    "q2b_save_results(q2_b, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3AfoRFzmFJs"
   },
   "source": [
    " ### <font color='red'> Inline Question 4 </font>\n",
    "\n",
    "*   PixelCNN++ uses descretized mixture of logistics instead of negative log likelihood. Please explain briefly what mixture of logistics is and why is it preferred when modeling distribution of images.\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7HKTrsUmQIt"
   },
   "source": [
    "## [15 Points] Part (c) GatedPixelCNN on MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSfHhdUDpxBl"
   },
   "source": [
    "We will again train on MNIST. This time, we do not need to normalize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVU9UM6Vpwcb"
   },
   "outputs": [],
   "source": [
    "dataset_path = '/content/data'\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root=dataset_path,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "test_data = datasets.MNIST(root=dataset_path,\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=transform)\n",
    "print(train_data)\n",
    "\n",
    "plt.imshow(train_data.data[447], cmap='gray')\n",
    "plt.title('%i' % train_data.targets[447])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jDx_sagxav8"
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXuggoE7mbOj"
   },
   "source": [
    "In this part, you will implement the gated version of the PixelCNN, which solves the blind spot problem of PixelCNN. Please take a look at [the paper](https://arxiv.org/pdf/1606.05328.pdf) for the detailed implementation.\n",
    "\n",
    "First, we will start by implementing a GatedConv2d layer, following Figure 2 of the paper: ![gated](https://drive.google.com/uc?export=view&id=1slOjRQeQaQSVDUCPj6ZsUPFXDQsD9kua)\n",
    "\n",
    "Please take a look at this figure and read the Gated Convolutional Layers part of the paper carefully to implement the following function. This layer should include 4 convolutions with different kernel sizes, as shown in the figure. Again, a header is provided to you, but you may wish to write your own class or your own functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj3kf-Irp-SB"
   },
   "outputs": [],
   "source": [
    "class GatedConv2d(nn.Module):\n",
    "  def __init__(self, mask_type, in_channels, out_channels, kernel_size=7, padding=3):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fhbuh1Bqym2"
   },
   "source": [
    "Now, you can use this layer inside your GatedPixelCNN++ model. Here is a pseudocode for the reference implementation:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "class GatedPixelCNN\n",
    "  MaskedConv2d(mask_type='A', in_channels = 1, out_channels = 120, kernel_size = 7, padding = 3)\n",
    "  BatchNorm(num_features=2*120)\n",
    "  ReLU()\n",
    "\n",
    "  for _ in range(3):\n",
    "    GatedConv2d(mask_type='B', in_channels = 120, out_channels = 120, kernel_size = 7, padding = 3)\n",
    "    BatchNorm(num_features=2*120)\n",
    "    ReLU()\n",
    "  \n",
    "  MaskedConv2d(mask_type='B', in_channels = 120, out_channels = 256, kernel_size = 7, padding = 3)\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zb8c4BO9qxhS"
   },
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "  def __init__(self, num_layers = 5, num_channels = 120, kernel_size=7):\n",
    "    \"\"\" YOUR CODE HERE \"\"\"\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmGklqr3srDq"
   },
   "source": [
    "Now you are ready to train your model. Fill in the function `q2_c` below to train the model and return the correct deliverables. In this function, you will initialize your model and train this model. The reference implementation uses `batch_size = 128`, `num_epocs = 3`, `optimizer = Adam`, `lr = 1e-3` as hyperparameters. Use Cross Entropy Loss to train this model, just like in Part (a). It should take around 60-65 minutes to train for 3 epochs\n",
    "\n",
    "**You will provide the following deliverables:**\n",
    "\n",
    "*   **A list of training losses evaluated at every minibatch. (Create an empty list before training, append the loss after every minibatch and return the final list).**\n",
    "*   **A list of test losses evaluated once at initialization and after each epoch. (Create an empty list. Evaluate on the test set before training and append it to this list. After every epoch, evaluate on the test set again and append to the list. Return the final list).**\n",
    "*   **25 samples drawn from your trained model. (The sampling function is already provided to you. Just call this function after training and return the samples).**\n",
    "\n",
    "Here is a pseudocode for the reference implementation. Once again, you may wish to implement a generic training function (`train_model`) that takes your model and dataloaders and returns the training losses, or use your implementations from previous parts:\n",
    "\n",
    "\n",
    "```\n",
    "function q2c():\n",
    "  batch_size = 128\n",
    "  num_epochs = 3\n",
    "  train_loader = data.DataLoader(train_data, ...)\n",
    "  test_loader = data.DataLoader(test_data, ...)\n",
    "  device = 'cuda:0' or 'cpu'\n",
    "  model = GatedPixelCNN().to(device)\n",
    "  train_losses, test_losses = train_model(model, train_loader, test_loader, ...)\n",
    "  \n",
    "  samples = sample_gated(model)\n",
    "  return train_losses, test_losses, samples\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfiEzCTOs_IL"
   },
   "outputs": [],
   "source": [
    "# Execute this cell, nothing to implement here\n",
    "def sample_gated(model):\n",
    "  \"\"\"\n",
    "  model: GatedPixelCNN model trained on MNIST\n",
    "\n",
    "  Returns\n",
    "  - sample: A tensor containing the samples generated by the model\n",
    "  \"\"\"\n",
    "  \n",
    "  no_images = 25\n",
    "  images_channels = 1\n",
    "  images_size = 28\n",
    "  sample = torch.Tensor(no_images, images_channels, images_size, images_size).cuda()\n",
    "  sample.fill_(0)\n",
    "\n",
    "  for i in range(images_size):\n",
    "    for j in range(images_size):\n",
    "      out = model(sample)\n",
    "      probs = F.softmax(out[:,:,i,j], dim=-1).data\n",
    "      sample[:,:,i,j] = torch.multinomial(probs, 1).float() / 255.0\n",
    "  return sample.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sj0lAtQtE9a"
   },
   "outputs": [],
   "source": [
    "def q2_c(train_data, test_data):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W) numpy array of greyscale MNIST images\n",
    "  test_data: A (n_test, H, W) numpy array of greyscale MNIST images\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) list of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) list of test_losses evaluated once at initialization and after each epoch\n",
    "  - a tensor containing samples generated by the trained model\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\" YOUR CODE HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHjta_HZtJ9l"
   },
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q2_c`, execute the cell below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Md315H3tNgs"
   },
   "outputs": [],
   "source": [
    "q2c_save_results(q2_c, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0trxd7Hmhmk"
   },
   "source": [
    " ### <font color='red'> Inline Question 5 </font>\n",
    "\n",
    "*   In question 2, you have implemented various PixelCNN models. You have seen the effective receptive fields of each of these models. It's time to visualize them. Please execute the nest cell to visualize receptive fields for PixelCNN and GatedPixelCNN (This cell should run with no issues if you have implemented the MaskedConv2d and GatedConv2d classes correctly, with the given arguments. If you have changed the input arguments etc. you might need to modify this cell.)\n",
    "*   Please briefly explain your observations. What is the difference between the PixelCNN and GatedPixelCNN in terms of these receptive fields? What causes this difference and why is this important?\n",
    "\n",
    "\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXgFGdAJjwLz"
   },
   "outputs": [],
   "source": [
    "def visualize_receptive_field():\n",
    "  layer1 = MaskedConv2d('A',1, 1, 3, 1, 1).cuda()\n",
    "  layer2 = MaskedConv2d('B',1, 1, 3, 1, 1).cuda()\n",
    "  gated_layer1 = GatedConv2d('A',1, 1, 3, 1).cuda()\n",
    "  gated_layer2 = GatedConv2d('B',1, 1, 3, 1).cuda()\n",
    "  x = torch.randn(1, 1, 11, 11).cuda()\n",
    "  x.requires_grad = True\n",
    "  x_g = torch.randn(1, 1, 11, 11).cuda()\n",
    "  x_g.requires_grad = True\n",
    "  nrows = 3\n",
    "  fig, ax = plt.subplots(nrows=nrows, ncols=2,figsize=(5,10))\n",
    "  for j,row in enumerate(ax):\n",
    "    out_g = torch.cat((x_g, x_g), dim=1)\n",
    "    out_g = gated_layer1(out_g)\n",
    "    out = layer1(x)\n",
    "    for i in range(j):\n",
    "      out = layer2(out)\n",
    "      out_g = gated_layer2(out_g)\n",
    "    out_g = out_g.chunk(2, dim=1)[1]\n",
    "    col1, col2 = row\n",
    "    plot_receptive_field(out, x, col1, ' PixelCNN', j+1)\n",
    "    plot_receptive_field(out_g, x_g, col2, ' GatedPixelCNN', j+1)\n",
    "    x.grad.zero_()\n",
    "    x_g.grad.zero_()\n",
    "\n",
    "visualize_receptive_field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [30 Points] Question 3: Causal Transformer - iGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move onto the current most popular and widespread autoregressive model, the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 Points] Part (a) Autoregressive Transformer on Shapes and MNIST\n",
    "In this part, implement a simple Autoregressive Transformer to model binary MNIST and shapes images (same as Q2(a), but with a Transformer). \n",
    "\n",
    "Some additional notes about your transformer implementation:\n",
    " * iGPT uses learned positional encodings. We recommend to use those here as well. However, you may also use sinusoidal positional encodings if you wish (see the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper)\n",
    " * Autoregressive transformer always predicts the **next** token, give prior tokens. iGPT has a special **\\<bos\\>** or beginning of sequence token at the start of every sequence every image. Make sure to include this in your implementation as well. You can generate unconditional sample by conditioning with the **\\<bos\\>** token.\n",
    " * While dropout is a common feature in transformer models, you do not need to add it (but may if you wish!).\n",
    " * Prebuilt transformers exist in some frameworks (i.e. pytorch). Don't just use an off the shelf implementation as the point of the exercise is to better understand the transformer architecture. Building the transformer from the ground up (use primitives such as Linear/Dense layers, LayerNorm, GeLU, Embedding)\n",
    " * Learning rate warmup and cos learning rate decay are often used when training transformers to improve training stability and improve performance. See if this helps your model! Try 1000 steps of warmup with a cosine learning rate decay.\n",
    "\n",
    "Paper references\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) \n",
    "* [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) \n",
    "* [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "We recommend the following network design parameters:\n",
    "* $d_{model}$: 128\n",
    "* heads: 4\n",
    "* layers: 2\n",
    "* GeLU nonlinearities\n",
    "\n",
    "And the following hyperparameters:\n",
    "* Batch size: 64 or 32 or 16 (whichever fits in your GPU)\n",
    "* Learning rate: $10^{-3}$\n",
    "* 15 epochs or more\n",
    "* Adam Optimizer (this applies to all Transformers models trained in future parts)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_a(train_data, test_data, image_shape, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, 1) uint8 numpy array of color images with values in {0, 1}\n",
    "  test_data: A (n_test, H, W, 1) uint8 numpy array of color images with values in {0, 1}\n",
    "  image_shape: (H, W, 1), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, W, 1) of samples with values in {0, 1}\n",
    "  \"\"\"\n",
    "  return train_losses, test_losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_a`, execute the cells below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3ab_save_results(1, 'a', q3_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3ab_save_results(2, 'a', q3_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color='red'> Inline Question 6 </font>\n",
    "\n",
    "*   Please explain briefly what is the motivation behind iGPT (1-2 sentences), how it is training objective works (2-3 sentences)? How can we check the performance of this unsupervised model (2-3 sentences)?\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 Points] Part (b) iGPT on Colored Shapes and MNIST\n",
    "\n",
    "Now, implement an iGPT that models color. In order to reduce the length of token sequences, iGPT models each RGB pixel as a **single** token. This effectively reduces the context length from H*W*C to just H*W. iGPT does this through a k-means clustering approach. Because our images only each can only take on 4 values (2 bits) per channel, we can represent each pixel with 64 values (6 bits). Convert the dataset into an image of tokens and train iGPT on the colored shapes and MNIST dataset.\n",
    "\n",
    "Checkout the iGPT paper for more details: [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) \n",
    "\n",
    "Training times and hyperparameter settings should be the same as part (a), except train for longer (15 epochs)\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1.   Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2.   Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q3_b(train_data, test_data, image_shape, dset_id):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, W, C) of samples with values in {0, 1, 2, 3}\n",
    "  \"\"\"\n",
    "  return train_losses, test_losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `q3_b`, execute the cells below to visualize and save your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3ab_save_results(1, 'b', q3_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3ab_save_results(2, 'b', q3_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### <font color='red'> Inline Question 7 </font>\n",
    "\n",
    "*   How might including colored shapes alongside the MNIST dataset in the training of iGPT affect the model's ability to generalize and learn representations, and what is the underlying intuition for this effect (explain in 2-3 sentences)?\n",
    "\n",
    "<font color='red'> Your Answers: </font> [Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [25 Points] Bonus: Causal Transformer: Tokenized Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Tokenization with Vector Quanization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [10 Points] Part (a) Image Quantization\n",
    "\n",
    "Above, we implemented iGPT, which autoregressivly predicts raw pixels. Transformers have quadratic complexity in the sequence length which prevents this naive approach from scaling well to large images.\n",
    "\n",
    "The space of natural images often contains very correlated information. This suggests we can learn a reduced representation. VQVAE is a method that does just that, learning to map images to a more compact discrete set of tokens. We will cover this method in more detail in future lectures. The only thing you need to know now is that we can learn an encoder (and corresponding decoder), which can extract a discrete representation from an image. \n",
    "\n",
    "_You can find the vqvae checkpoints [here](https://drive.google.com/drive/folders/1Gfk-OOzmOXQ0J3WbQU7jy99Xk4gNfirw?usp=sharing). You must put them inside your `./data/hw1_data/` folder._\n",
    "\n",
    "If you are curious, checkout the VQVAE paper to learn more: https://arxiv.org/abs/1711.00937 (we will cover this in a future lecture though!)\n",
    "\n",
    "In this part, we provide a pre-trained VQVAE model, which consists of:\n",
    " * encoder to tokenize the images\n",
    " * the decoder to recover the image\n",
    " * a token vocabulary of VQVAE_MODEL.n_embeddings\n",
    "\n",
    "Below is the code for loading the VQ model. Note that VQVAE encoding process is lossy, so the decoded images will not be the exact same as the input. Some blurriness in the recovered image is to be expected. The docstrings of the relevant methods you will need for the VQVAE_MODEL are provided below for your convenience. \n",
    "\n",
    "We will use 2 colored mnist datasets in this part. The first is the same dataset used in previous parts. The second, hads a colored digit on a differently colored background. We will call these datasets Colored MNIST and Colored MNIST v2. Note that the vqvae is trained per dataset.\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Use the provided encoder model to quantize the images then inspect the recovered images by applying the decoder for each of the two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @property\n",
    "# def n_embeddings(self) -> int:\n",
    "#     \"\"\"The size of the token vocabulary\"\"\"\n",
    "#    \n",
    "# def quantize(self, x: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Quantize an image x.\n",
    "#\n",
    "#     Args:\n",
    "#         x (np.ndarray, dtype=int): Image to quantize. shape=(batch_size, 28, 28, 3). Values in [0, 3].\n",
    "#\n",
    "#     Returns:\n",
    "#         np.ndarray: Quantized image. shape=(batch_size, 7, 7). Values in [0, n_embeddings]\n",
    "#     \"\"\"\n",
    "#    \n",
    "# def decode(self, z_index: np.ndarray) -> np.ndarray:\n",
    "#     \"\"\"Decode a quantized image.\n",
    "#\n",
    "#     Args:\n",
    "#         z_index (np.ndarray, dtype=int): Quantized image. shape=(batch_size, 7, 7). Values in [0, n_embeddings].\n",
    "#\n",
    "#     Returns:\n",
    "#         np.ndarray: Decoded image. shape=(batch_size, 28, 28, 3). Values in [0, 3].\n",
    "#     \"\"\"\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_a(images, vqvae):\n",
    "  \"\"\"\n",
    "  images: (B, H, W, C), the images to pass through the encoder and decoder of the vqvae\n",
    "  vqvae: a vqvae model, trained on the relevant dataset\n",
    "\n",
    "  Returns\n",
    "  - a numpy array of size (2, H, W, C) of the decoded image\n",
    "  \"\"\"\n",
    "  return autoencoded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusa_save_results(1, bonus_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusa_save_results(2, bonus_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [15 Points] Part (b) Autoregressive Transformer on Colored Shapes and MNIST with Vector Quantization\n",
    "\n",
    "We can use the VQVAE to tokenize an image dataset. This will result in a much smaller sequence length than the approach we tried in Question 3(b). For this part, train a transformer on the dataset tokenized by the VQVAE.\n",
    "\n",
    "This is a simplified version of the approach used in VQGAN [VQGAN](https://arxiv.org/abs/2012.09841) -> Section 3.2: Learning the Composition of Images with Transformers (Again, we will cover this in more detail in a future lecture!)\n",
    "\n",
    "Update the following hyperparameters:\n",
    "* layers: 4 (we can train a bigger transformer now since less memory is used per input!)\n",
    "* 30 epochs\n",
    "\n",
    "**You will provide these deliverables**\n",
    "\n",
    "1. Over the course of training, record the average negative log-likelihood (nats / dim) of the training data (per minibatch) and test data (for your entire test set). Code is provided that automatically plots the training curves. \n",
    "2. Report the final test set performance of your final model\n",
    "3. 100 samples from the final trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bonus_b(train_data, test_data, image_shape, dset_id, vqvae):\n",
    "  \"\"\"\n",
    "  train_data: A (n_train, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  test_data: A (n_test, H, W, C) uint8 numpy array of color images with values in {0, 1, 2, 3}\n",
    "  image_shape: (H, W, C), height, width, and # of channels of the image\n",
    "  dset_id: An identifying number of which dataset is given (1 or 2). Most likely\n",
    "           used to set different hyperparameters for different datasets\n",
    "  vqvae: a vqvae model, trained on dataset dset_id\n",
    "\n",
    "  Returns\n",
    "  - a (# of training iterations,) numpy array of train_losses evaluated every minibatch\n",
    "  - a (# of epochs + 1,) numpy array of test_losses evaluated once at initialization and after each epoch\n",
    "  - a numpy array of size (100, H, C, W) of samples with values in {0, 1, 2, 3}\n",
    "  \"\"\"\n",
    "  return train_losses, test_losses, samples\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Once you've implemented `bonus_b`, execute the cells below to visualize and save your results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusb_save_results(1, bonus_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonusb_save_results(2, bonus_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What and How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Save your notebook and clear all the cells: Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells).\n",
    "2. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "3. Once you've rerun everything, select File -> Download as -> PDF via LaTeX (If you have trouble using \"PDF via LaTex\", you can also save the webpage as pdf. Make sure all your solutions especially the coding parts are displayed in the pdf, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
    "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. Name your pdf as **username_assignment1.pdf**, e.g. eacikgoz_assignment1.pdf.\n",
    "5. Download a .ipynb version of your notebook, please name this as **username_assignment1.ipynb**, e.g. eacikgoz17_assignment1.ipynb.\n",
    "6. You must submit both of your **username_assignment1.pdf** and **username_assignment1.ipynb** files to the Blackboard, before the deadline. Please note that failing to submit a PDF version of your Jupyter notebook, by following the steps above, will result in a deduction of points.\n",
    "7. Finally, submit your overleaf report in PDF format as **username_assignment1_report.pdf**.\n",
    "\n",
    "At the end, you msut have three different deliverables from Blackboard: (i) your jupyter notebook, (ii) PDF version of your jupyter notebook, (iii) PDF version of your overleaf report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQSzOjn4Vkq"
   },
   "source": [
    "*This assignment is adapted from UC Berkeley [CS294-158-SP20](https://sites.google.com/view/berkeley-cs294-158-sp20/home)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "HW1_Autoregressive_Models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
